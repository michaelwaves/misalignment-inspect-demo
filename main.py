from inspect_ai import Task, task, eval
from inspect_ai.model import get_model, GenerateConfig
from inspect_ai.dataset import json_dataset
from inspect_ai.solver import generate, system_message, use_tools
from tools import tools
from scorers import scorers


MODEL_NAME = "openai/gpt-4o-mini"
ENVIRONMENT_NAME = "blackmail_no_tools"
USE_TOOLS = False
RUNS_PER_SAMPLE=1

with open("system_prompt.md", "r") as f:
    system_prompt = f.read()
model = get_model(MODEL_NAME, config=GenerateConfig(system_message=system_prompt))


observed_dataset = json_dataset(f"environments/{ENVIRONMENT_NAME}/observed.json")
unobserved_dataset = json_dataset(f"environments/{ENVIRONMENT_NAME}/unobserved.json")

@task
def observed_task():
    return Task(
        name="observed_task",
        dataset=observed_dataset,
        scorer=scorers,
    )

@task 
def unobserved_task():
    return Task(
        name="unobserved_task",
        dataset=unobserved_dataset,
        scorer=scorers,
    )


if __name__ == "__main__":
    if USE_TOOLS:
        solver = [use_tools(tools), generate()]
        sandbox_kwargs = {"sandbox": "docker"}
    else:
        solver = [generate()]
        sandbox_kwargs = {}

    eval(
        tasks=[observed_task(), unobserved_task()],
        model=model,
        solver=solver,
        log_format="json",
        log_dir= f"logs/{MODEL_NAME.replace('/','_')}/{ENVIRONMENT_NAME}",
        message_limit=50,
        max_tokens=10000,
        epochs=RUNS_PER_SAMPLE,
        **sandbox_kwargs
    )
